---
title: "Core Model"
description: "Learn about the three core components to AgentPro voice AI pipeline."
---

At its core Indigenius is an orchestration layer over three modules: the **_transcriber_**, the **_model_**, and the **_voice_**.

![Agent Core Model Agent Gi](/images/core-model.png)

These three modules can be swapped out with any provider of your choosing: **OpenAI, Groq, Deepgram, ElevenLabs**, etc. You can even plug in your own server to act as the LLM.

Indigenius takes these three modules, optimizes latency, manages scaling & streaming, and orchestrates the conversation flow to make interactions sound human.

---

## 1. Listen (Intake Raw Audio)

When a person speaks, the client device (laptop, phone, etc.) records raw audio (1’s & 0’s at its core).  

This raw audio must either:

- Be transcribed on the **client device**, or  
- Be sent to a **server** to convert it into text.

---

## 2. Run an LLM

The transcribed text is then fed into a **prompt** and run through a **Large Language Model (LLM)**.  

The LLM acts as the core intelligence that simulates a human behind the scenes.

---

## 3. Speak (Text → Raw Audio)

The LLM outputs text, which must now be spoken. The text is converted into **raw audio** (again, 1’s & 0’s), which is playable on the user’s device.  

This process can happen:

- Directly on the **user’s device**, or  
- On a **server**, which sends the audio back to the client.

---

Indigenius orchestrates all these steps, ensuring a **smooth and responsive conversation**, while providing a simple set of tools to manage these internal processes.

---

## Orchestration Models

Indigenius runs real-time models on top of **STT**, **LLM**, and **TTS**.  

It also provides extra capabilities on top of the core models, including latency optimization and seamless streaming.

---

### High-Level Architecture Overview

| Module   | Function                                                                                   | Notes                                                                                       |
|---------|--------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------|
| **Listen (STT)** | Intake and transcribe raw audio from user devices                                       | Can be local or server-side                                                               |
| **LLM**         | Process text and generate conversational responses                                       | Acts as the “intelligence” behind the conversation                                        |
| **Speak (TTS)** | Convert LLM output into raw audio for playback                                         | Can happen locally or via server; optimized for low latency                                |
| **Orchestration** | Coordinates the flow between STT, LLM, and TTS                                        | Handles scaling, streaming, and latency optimization                                       |

---

Indigenius also runs a suite of **audio and text models** that make its **STT → LLM → TTS pipeline** feel natural and human-like.

This orchestration ensures real-time performance, low latency, and a seamless conversation experience.

Here is a high level pictogram of the AgentPro architecture:

![Agent Core Model Agent Gi](/images/core-model-high-level-pic.png)

